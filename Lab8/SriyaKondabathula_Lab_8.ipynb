{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Data\n",
        "\n",
        "1. Load mnist_test.csv from https://www.kaggle.com/datasets/oddrationale/mnist-in-csv?select=mnist_test.csv as data.\n",
        "\n",
        "2. Split data into X and y. X should have the shape as (10000,784) and y should have the shape as (10000,1).\n",
        "\n",
        "3. Split X and y into the train set (80%) and the test set (20%). The train set is for fitting your model while the test set is for evaluating your model. As a result, you will have X_train.shape as (8000,784), y_train.shape as (8000,1), X_test.shape as (2000,784),and y_test.shape as (2000,1). (Hint: use sklearn.model_selection.train_test_split.) "
      ],
      "metadata": {
        "id": "qtt-crrf_Kth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "data_pd = pd.read_csv('mnist_test.csv')\n",
        "data = np.array(data_pd)\n",
        "\n",
        "X = data[:,1:]\n",
        "y = data[:,0]\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "OS8Q_A0PB10T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7ff39fa-ea3d-4680-922d-122b81feb49b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 784)\n",
            "(10000,)\n",
            "(8000, 784)\n",
            "(2000, 784)\n",
            "(8000,)\n",
            "(2000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fitting and Evaluating Your Model\n",
        "1. Use sklearn.linear_model.RidgeClassifier to fit X_train and y_train to get a multi-class classification model. \n",
        "2. Test your model on (X_test, y_test) and get testing accuracy by using clf.score(X_test, y_test) assuming your model is named as \"clf\"."
      ],
      "metadata": {
        "id": "ZfyYzOYAB5gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import RidgeClassifier\n",
        "\n",
        "clf = RidgeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "test_accuracy = clf.score(X_test, y_test)\n",
        "print(test_accuracy)\n"
      ],
      "metadata": {
        "id": "vG6jxQQYefDR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48f4c1ae-c76c-4a9b-b054-8728232dd6f3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizing the RidgeClassifier\n",
        "1. In sklearn.linear_model.RidgeClassifier, there is one argument called \"alpha\" corresponding to the coefficient for the regularization. By default, alpha is equal to 1. There are benefits and drawbacks to having a large alpha. The larger is the alpha, the more likely you are going to have underfitting problems with your graph. Higher alpha does not necessarily mean better results. On the other hand, a low alpha may lead to overfitting problems and a more complicated model. \n",
        "\n",
        "  More information: https://towardsdatascience.com/preventing-overfitting-with-lasso-ridge-and-elastic-net-regularization-in-machine-learning-d1799b05d382\n",
        "\n",
        "  Please try different alpha to train your model and evaluate your model's test accuracy. Note: you cannot try number ranges such as (1-10) or (1-50), these numbers are too similar. (Hint: you'll want try alphas that are different powers of 10.) Out of what you chose, what is the best choice for alpha in MNIST classification?\n",
        "\n",
        "2. Instead of fitting the full dimension (784) of data to the RidgeClassifier, you can apply PCA to the data (PCA over X with the shape 10000*784) to reduce the dimension from 784 to 100 (for example) and train another RidgeClassifier with 100-dimension features. \n",
        "\n",
        "  Typically, we want the explained variance to be between 95–99% (which is what we would set n_components to). With alpha=1, iterate through the array 0.95-0.99 ([0.95, 0.96, ...., 0.99]) and set n_components to the value you're currently on in the array. Each time, print the shape of X_reduced to get the number of components that are left from the second value in the tuple. For example, (10000, 168) has 168 components left. For further explanation, see the scikit learn PCA function documentation: \n",
        "  > if 0 < n_components < 1 and svd_solver == ‘full’, select the number of components such that the amount of variance that needs to be explained is greater than the percentage specified by n_components\n",
        " \n",
        "  Then, set n_components to 784 and run again. I should see the results from 0.95-0.99 variance as well as the result of running n_components = 784 in your answer. Then answer this question: what is the best reduced dimension number of components to get the highest test accuracy? (Hint: after applying PCA to 10000 samples, remember to split train and test.)\n",
        "  \n",
        "  More information on variance and PCA:  https://stackoverflow.com/questions/32857029/python-scikit-learn-pca-explained-variance-ratio-cutoff\n"
      ],
      "metadata": {
        "id": "1W1rxcCZegU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 \n",
        "\n",
        "# Init best accuracy performance\n",
        "acc_best = 0\n",
        "for alpha in [10,100,1000, 10000, 100000]: \n",
        "  clf = RidgeClassifier(alpha=alpha)\n",
        "  clf.fit(X_train, y_train)\n",
        "\n",
        "  # Check classifier's accuracy on the test set\n",
        "  test_acc = clf.score(X_test, y_test)\n",
        "  print(f\"For {alpha}, acc: {test_acc}\")\n",
        "\n",
        "  # Update the best choice of alpha according to the test accuracy\n",
        "  if test_acc > acc_best:\n",
        "    acc_best = test_acc\n",
        "    clf_best = clf\n",
        "print(f\"Best choice of alpha is {alpha} with acc: {clf_best.score(X_test, y_test)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "2OK0bA4IrY47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f506cea9-b206-4055-cda9-cf7e8c54fd13"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For 10, acc: 0.8495\n",
            "For 100, acc: 0.851\n",
            "For 1000, acc: 0.85\n",
            "For 10000, acc: 0.851\n",
            "For 100000, acc: 0.856\n",
            "Best choice of alpha is 100000 with acc: 0.856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 \n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "for i in [0.95, 0.96, 0.97, 0.98, 0.99, 784]:\n",
        "  pca = PCA(n_components = i)\n",
        "  data_reduced = pca.fit_transform(X)\n",
        "  target = y\n",
        "  print(data_reduced.shape)\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(data_reduced,target, test_size=0.2, random_state=42)\n",
        "\n",
        "  clf = RidgeClassifier(alpha= 1)\n",
        "  clf.fit(X_train, y_train)\n",
        "\n",
        "  # Check classifier's accuracy on the test set\n",
        "  test_acc = clf.score(X_test, y_test)\n",
        "  print(f\"For {i}, acc: {test_acc}\")\n",
        "\n",
        "print(\"(10000,253) is the best reduced dimension number of components to get the highest test accuracy of 86.35%\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZhcY2LrWsDt",
        "outputId": "0c9a6145-c527-4021-f691-3e2b4f9437b7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 149)\n",
            "For 0.95, acc: 0.8615\n",
            "(10000, 174)\n",
            "For 0.96, acc: 0.8605\n",
            "(10000, 207)\n",
            "For 0.97, acc: 0.863\n",
            "(10000, 253)\n",
            "For 0.98, acc: 0.8635\n",
            "(10000, 323)\n",
            "For 0.99, acc: 0.8595\n",
            "(10000, 784)\n",
            "For 784, acc: 0.849\n",
            "(10000,253) is the best reduced dimension number of components to get the highest test accuracy of 86.35%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix and Classification Report\n",
        "1. Read https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html to understand how to use a confusion matrix. Based on the information you learned from #1, can you plot the confusion matrix accordingly? (Hint: use clf.predict(X_test) to get the prediction labels over X_test.)\n",
        "\n",
        "2. Read https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report to understand how to use a classification report. Based on the information you learned in #3, can you output the classification report accordingly? What is the label with the lowest precision?"
      ],
      "metadata": {
        "id": "mitvXX17remR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "FxhAoKrTrgLq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02f526ab-36b4-4a41-eaf9-822ab265b6cd"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[193   0   1   2   0   3   3   0   1   0]\n",
            " [  0 211   2   0   0   0   2   0   1   0]\n",
            " [  3  10 171   4   4   2   8   3   6   2]\n",
            " [  1   7   4 178   1   5   1   5   2   4]\n",
            " [  0   1   5   1 184   2   6   0   3  13]\n",
            " [  2   2   2  10   6 125   5   3  12   7]\n",
            " [  6   1   1   1   3   5 180   2   1   0]\n",
            " [  1   8   1   1   7   0   0 161   0   8]\n",
            " [  1   6   5  11   5   6   2   7 140   3]\n",
            " [  4   3   0   3  15   1   0  15   2 155]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.95      0.93       203\n",
            "           1       0.85      0.98      0.91       216\n",
            "           2       0.89      0.80      0.84       213\n",
            "           3       0.84      0.86      0.85       208\n",
            "           4       0.82      0.86      0.84       215\n",
            "           5       0.84      0.72      0.77       174\n",
            "           6       0.87      0.90      0.88       200\n",
            "           7       0.82      0.86      0.84       187\n",
            "           8       0.83      0.75      0.79       186\n",
            "           9       0.81      0.78      0.79       198\n",
            "\n",
            "    accuracy                           0.85      2000\n",
            "   macro avg       0.85      0.85      0.85      2000\n",
            "weighted avg       0.85      0.85      0.85      2000\n",
            "\n"
          ]
        }
      ]
    }
  ]
}